{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "07lPvV04CvNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11d7e54-4d20-40de-b9b8-82e92d93ac2d"
      },
      "source": [
        "# Instalação de dependências no Colab\n",
        "!pip install mediapipe opencv-python tqdm deepface transformers sentencepiece --quiet\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/10.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/10.3 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/133.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": 1,
      "id": "07lPvV04CvNQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb5is8Ex4RsT",
        "outputId": "ce44c2bb-1ff6-4b27-e6e3-145b77419ddb"
      },
      "id": "Cb5is8Ex4RsT",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_DDWFv4CvNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fc80fc-ce82-485a-fc48-caf16a240664"
      },
      "source": [
        "\"\"\"Analisador de vídeo com IA (faces, emoções e ações) totalmente em PT-BR.\n",
        "\n",
        "- Detecta rostos em cada frame de um vídeo (MediaPipe Tasks).\n",
        "- Analisa a emoção de cada rosto (DeepFace) e traduz para português.\n",
        "- Gera uma legenda descritiva da cena (BLIP) e traduz EN->PT-BR.\n",
        "- Atribui um identificador único por pessoa ao longo do vídeo (pessoa1, pessoa2, ...).\n",
        "- Gera:\n",
        "  - Um vídeo anotado com bounding boxes e textos.\n",
        "  - Um CSV detalhado (frame a frame).\n",
        "  - Um CSV de resumo com uma emoção e uma ação representativa por pessoa.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "from deepface import DeepFace\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        ")\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------\n",
        "# Configuração de logging\n",
        "# ---------------------------\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger(\"video_emotion_analysis\")\n",
        "\n",
        "# Tipo auxiliar para bounding box de face (x, y, largura, altura)\n",
        "FaceBox = Tuple[int, int, int, int]\n",
        "\n",
        "\n",
        "FACE_MODEL_URL = \"https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/latest/blaze_face_short_range.tflite\"\n",
        "FACE_MODEL_PATH = \"blaze_face_short_range.tflite\"\n",
        "\n",
        "def ensure_face_model_downloaded(model_path: str = FACE_MODEL_PATH) -> str:\n",
        "    \"\"\"Garante que o modelo de detecção facial do MediaPipe esteja disponível localmente.\n",
        "\n",
        "    Faz o download automático do modelo se ele ainda não existir no diretório atual.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(model_path):\n",
        "        import urllib.request\n",
        "        logger.info(\"Baixando modelo de detecção facial do MediaPipe...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(FACE_MODEL_URL, model_path)\n",
        "            logger.info(f\"Modelo salvo em: {model_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Falha ao baixar o modelo de face do MediaPipe: {e}\")\n",
        "            raise\n",
        "    return model_path\n",
        "\n",
        "# # ---------------------------\n",
        "# # Inicialização de modelos\n",
        "# # ---------------------------\n",
        "# def init_mediapipe_face_detector(conf=0.5):\n",
        "#     mp_face_detection = mp.solutions.face_detection\n",
        "#     face_det = mp_face_detection.FaceDetection(min_detection_confidence=conf)\n",
        "#     return face_det\n",
        "\n",
        "def init_mediapipe_face_detector(conf=0.5):\n",
        "    \"\"\"Inicializa o detector facial usando a API moderna `MediaPipe Tasks`.\n",
        "\n",
        "    Compatível com as versões mais recentes do pacote `mediapipe` no Colab.\n",
        "    \"\"\"\n",
        "    # Garante o modelo local\n",
        "    model_path = ensure_face_model_downloaded()\n",
        "\n",
        "    # Atalhos das classes da API de Tasks\n",
        "    BaseOptions = mp.tasks.BaseOptions\n",
        "    FaceDetector = mp.tasks.vision.FaceDetector\n",
        "    FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
        "    VisionRunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "    options = FaceDetectorOptions(\n",
        "        base_options=BaseOptions(model_asset_path=model_path),\n",
        "        running_mode=VisionRunningMode.IMAGE,  # processa frame a frame\n",
        "        min_detection_confidence=conf,\n",
        "    )\n",
        "    detector = FaceDetector.create_from_options(options)\n",
        "    return detector\n",
        "\n",
        "def init_blip_colab(device=\"cpu\"):\n",
        "    try:\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        if device != \"cpu\":\n",
        "            model.to(device)\n",
        "        return processor, model\n",
        "    except Exception as e:\n",
        "        logger.warning(\"BLIP não pôde ser carregado. Caption desativado. Erro: %s\", e)\n",
        "        return None, None\n",
        "\n",
        "# ---------------------------\n",
        "# Emoções com DeepFace\n",
        "# ---------------------------\n",
        "def analyze_emotion(face_img):\n",
        "    \"\"\"Analisa emoções com DeepFace e normaliza o retorno.\n",
        "\n",
        "    Retorno normalizado:\n",
        "    {\n",
        "        \"dominant_emotion\": <str | None>,\n",
        "        \"emotion\": <dict de scores>\n",
        "    }\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = DeepFace.analyze(\n",
        "            face_img,\n",
        "            actions=['emotion'],\n",
        "            enforce_detection=False\n",
        "        )\n",
        "\n",
        "        # DeepFace pode retornar uma lista (uma entrada por face)\n",
        "        if isinstance(result, list):\n",
        "            if len(result) == 0:\n",
        "                return {\"dominant_emotion\": None, \"emotion\": {}}\n",
        "            result = result[0]\n",
        "\n",
        "        dominant = result.get(\"dominant_emotion\")\n",
        "        emotions = result.get(\"emotion\", {})\n",
        "\n",
        "        return {\n",
        "            \"dominant_emotion\": dominant,\n",
        "            \"emotion\": emotions\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Erro na análise de emoção com DeepFace: {e}\")\n",
        "        return {\"dominant_emotion\": None, \"emotion\": {}}\n",
        "\n",
        "# ---------------------------\n",
        "# Tradução de emoções para PT-BR\n",
        "# ---------------------------\n",
        "EMOTION_PT = {\n",
        "    \"angry\": \"Raiva\",\n",
        "    \"disgust\": \"Nojo\",\n",
        "    \"fear\": \"Medo\",\n",
        "    \"happy\": \"Alegre\",\n",
        "    \"sad\": \"Triste\",\n",
        "    \"surprise\": \"Surpreso\",\n",
        "    \"neutral\": \"Neutro\",\n",
        "}\n",
        "\n",
        "def translate_emotion(emotion: Optional[str]) -> str:\n",
        "    \"\"\"Traduz o rótulo de emoção do DeepFace para PT-BR.\n",
        "\n",
        "    Se não houver emoção, retorna \"N/A\".\n",
        "    \"\"\"\n",
        "    if not emotion:\n",
        "        return \"N/A\"\n",
        "    # Garante string e aplica lower para mapear no dicionário\n",
        "    key = str(emotion).lower()\n",
        "    return EMOTION_PT.get(key, str(emotion))\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Detector MediaPipe\n",
        "# ---------------------------\n",
        "# def detect_faces_mediapipe(frame_rgb, detector):\n",
        "#     h, w, _ = frame_rgb.shape\n",
        "#     results = detector.process(frame_rgb)\n",
        "#     faces = []\n",
        "\n",
        "#     if results.detections:\n",
        "#         for det in results.detections:\n",
        "#             box = det.location_data.relative_bounding_box\n",
        "#             x = int(box.xmin * w)\n",
        "#             y = int(box.ymin * h)\n",
        "#             bw = int(box.width * w)\n",
        "#             bh = int(box.height * h)\n",
        "#             x = max(0, x); y = max(0, y)\n",
        "#             faces.append({\"bbox\": (x, y, bw, bh), \"score\": float(det.score[0])})\n",
        "#     return faces\n",
        "def detect_faces_mediapipe(frame_rgb, detector):\n",
        "    \"\"\"Detecta faces em um frame RGB usando MediaPipe Tasks.\n",
        "\n",
        "    Retorna uma lista de dicionários:\n",
        "        { \"bbox\": (x, y, w, h), \"score\": float }\n",
        "    em coordenadas de pixels.\n",
        "    \"\"\"\n",
        "    h, w, _ = frame_rgb.shape\n",
        "    faces = []\n",
        "\n",
        "    # Converte o frame (numpy array) para `mp.Image`\n",
        "    try:\n",
        "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Falha ao criar mp.Image para MediaPipe: {e}\")\n",
        "        return faces\n",
        "\n",
        "    try:\n",
        "        result = detector.detect(mp_image)\n",
        "    except AttributeError:\n",
        "        # Fallback: se por algum motivo o detector ainda for da API legada (mp.solutions),\n",
        "        # mantém compatibilidade com o .process(...)\n",
        "        results = detector.process(frame_rgb)\n",
        "        if results.detections:\n",
        "            for det in results.detections:\n",
        "                box = det.location_data.relative_bounding_box\n",
        "                x = int(box.xmin * w)\n",
        "                y = int(box.ymin * h)\n",
        "                bw = int(box.width * w)\n",
        "                bh = int(box.height * h)\n",
        "                x = max(0, x); y = max(0, y)\n",
        "                faces.append({\"bbox\": (x, y, bw, bh), \"score\": float(det.score[0])})\n",
        "        return faces\n",
        "\n",
        "    # API nova (Tasks) -> FaceDetectorResult\n",
        "    if not result.detections:\n",
        "        return faces\n",
        "\n",
        "    for det in result.detections:\n",
        "        try:\n",
        "            # bounding_box já vem em coordenadas de pixels\n",
        "            box = det.bounding_box\n",
        "            x = int(box.origin_x)\n",
        "            y = int(box.origin_y)\n",
        "            bw = int(box.width)\n",
        "            bh = int(box.height)\n",
        "\n",
        "            score = None\n",
        "            if getattr(det, \"categories\", None):\n",
        "                score = float(det.categories[0].score)\n",
        "            elif getattr(det, \"score\", None):\n",
        "                score = float(det.score[0])\n",
        "\n",
        "            x = max(0, x); y = max(0, y)\n",
        "            faces.append({\"bbox\": (x, y, bw, bh), \"score\": float(score) if score is not None else 0.0})\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Falha ao extrair bounding box de uma detecção do MediaPipe: {e}\")\n",
        "\n",
        "    return faces\n",
        "\n",
        "# ---------------------------\n",
        "# Caption com BLIP\n",
        "# ---------------------------\n",
        "def generate_caption(frame_bgr, processor, model, device=\"cpu\"):\n",
        "    \"\"\"Gera legenda em inglês usando BLIP.\"\"\"\n",
        "    if processor is None or model is None:\n",
        "        return None\n",
        "    try:\n",
        "        import torch\n",
        "        rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "        inputs = processor(images=rgb, return_tensors=\"pt\").to(device)\n",
        "        out = model.generate(**inputs)\n",
        "        return processor.decode(out[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Erro ao gerar legenda com BLIP: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---------------------------\n",
        "#  Remover acentos e especiais\n",
        "# ---------------------------\n",
        "def remove_accents(texto):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', texto)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def sem_acentos(txt: str) -> str:\n",
        "    # NFKD separa letra do acento (ex: \"ã\" -> \"a\" + \"~\"); depois removemos os acentos (combining marks)\n",
        "    txt = unicodedata.normalize(\"NFKD\", txt)\n",
        "    txt = \"\".join(ch for ch in txt if not unicodedata.combining(ch))\n",
        "    # Alguns símbolos comuns que podem aparecer e não renderizam bem no Hershey:\n",
        "    txt = (txt.replace(\"–\", \"-\")\n",
        "              .replace(\"—\", \"-\")\n",
        "              .replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "              .replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
        "    return txt\n",
        "\n",
        "# ---------------------------\n",
        "# Tradução de legendas EN->PT-BR (offline HuggingFace)\n",
        "# ---------------------------\n",
        "def init_translator_en_pt(device: str = \"cpu\"):\n",
        "    \"\"\"Inicializa modelo unicamp-dl/translation-en-pt-t5 para tradução EN->PT.\n",
        "\n",
        "    Usa apenas modelos locais da biblioteca transformers (sem API externa).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model_name = \"unicamp-dl/translation-en-pt-t5\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        if device != \"cpu\":\n",
        "            model.to(device)\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Erro ao carregar modelo de tradução EN->PT: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def translate_caption_pt(caption: Optional[str],\n",
        "                         tokenizer,\n",
        "                         model,\n",
        "                         device: str = \"cpu\") -> Optional[str]:\n",
        "    \"\"\"Traduz a legenda gerada em inglês para PT-BR.\n",
        "\n",
        "    Se não for possível traduzir, retorna a legenda original.\n",
        "    \"\"\"\n",
        "    if not caption or tokenizer is None or model is None:\n",
        "        return caption\n",
        "    try:\n",
        "        import torch\n",
        "        inputs = tokenizer(caption, return_tensors=\"pt\", truncation=True, max_length=64)\n",
        "        if device != \"cpu\":\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=64)\n",
        "        pt_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return pt_text\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Erro ao traduzir legenda EN->PT: {e}\")\n",
        "        return caption\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Anotar frame\n",
        "# ---------------------------\n",
        "def draw_label(frame, bbox, label, score=None):\n",
        "    x, y, w, h = bbox\n",
        "    cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)\n",
        "    text = label if score is None else f\"{label} ({score:.2f})\"\n",
        "    cv2.putText(frame, text, (x, y-8), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
        "\n",
        "# ---------------------------\n",
        "# Pipeline principal\n",
        "# ---------------------------\n",
        "def process_video_colab(input_path, output_path, csv_path, skip_frames=0, conf=0.5):\n",
        "\n",
        "    # Detecção facial + BLIP\n",
        "    face_detector = init_mediapipe_face_detector(conf)\n",
        "    processor, blip_model = init_blip_colab(device=\"cpu\")\n",
        "\n",
        "    # Tradutor EN->PT para legendas\n",
        "    translator_tokenizer, translator_model = init_translator_en_pt(device=\"cpu\")\n",
        "\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps_in = cap.get(cv2.CAP_PROP_FPS) or 25\n",
        "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    out_writer = cv2.VideoWriter(\n",
        "        output_path,\n",
        "        cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "        fps_in,\n",
        "        (w, h)\n",
        "    )\n",
        "\n",
        "    csv_data = []\n",
        "    start = time.time()\n",
        "\n",
        "    pbar = tqdm(total=total_frames, desc=\"Processando vídeo\")\n",
        "\n",
        "    frame_idx = 0\n",
        "    processed = 0\n",
        "    person_counter = 0  # contador global de pessoas detectadas\n",
        "    person_trackers = []  # rastreadores simples por proximidade (id, cx, cy)\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if skip_frames and (frame_idx % (skip_frames + 1) != 0):\n",
        "            frame_idx += 1\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        faces = detect_faces_mediapipe(frame_rgb, face_detector)\n",
        "\n",
        "        # Legenda em inglês via BLIP + tradução para PT-BR\n",
        "        caption_en = generate_caption(frame, processor, blip_model)\n",
        "        caption = translate_caption_pt(caption_en, translator_tokenizer, translator_model, device=\"cpu\")\n",
        "\n",
        "        if not faces:\n",
        "            csv_data.append({\n",
        "                \"frame\": frame_idx,\n",
        "                \"face_id\": -1,\n",
        "                \"person_label\": None,\n",
        "                \"emotion\": None,\n",
        "                \"score\": None,\n",
        "                \"caption\": caption\n",
        "            })\n",
        "\n",
        "        for i, f in enumerate(faces):\n",
        "            # Rastreamento simples: associa cada face à pessoa mais próxima (mesmo id ao longo do vídeo)\n",
        "            x, y, w2, h2 = f[\"bbox\"]\n",
        "            cx = x + w2 / 2.0\n",
        "            cy = y + h2 / 2.0\n",
        "\n",
        "            best_id = None\n",
        "            best_dist = 1e12\n",
        "            best_idx = None\n",
        "            for idx, t in enumerate(person_trackers):\n",
        "                dx = cx - t[\"cx\"]\n",
        "                dy = cy - t[\"cy\"]\n",
        "                dist = dx*dx + dy*dy\n",
        "                if dist < best_dist:\n",
        "                    best_dist = dist\n",
        "                    best_id = t[\"id\"]\n",
        "                    best_idx = idx\n",
        "\n",
        "            # limiar de distância em pixels (quadrado)\n",
        "            dist_thresh_sq = 80 * 80\n",
        "            if best_id is not None and best_dist <= dist_thresh_sq:\n",
        "                pid = best_id\n",
        "                # atualiza posição do rastreador\n",
        "                person_trackers[best_idx][\"cx\"] = cx\n",
        "                person_trackers[best_idx][\"cy\"] = cy\n",
        "            else:\n",
        "                # nova pessoa\n",
        "                person_counter += 1\n",
        "                pid = person_counter\n",
        "                person_trackers.append({\"id\": pid, \"cx\": cx, \"cy\": cy})\n",
        "\n",
        "            person_label = f\"pessoa{pid}\"\n",
        "\n",
        "            crop = frame[y:y+h2, x:x+w2]\n",
        "\n",
        "            result = analyze_emotion(crop)\n",
        "            emo = result.get(\"dominant_emotion\")\n",
        "            scores = result.get(\"emotion\", {}) or {}\n",
        "\n",
        "            # Fallback: se não vier dominant_emotion, escolhe a maior probabilidade\n",
        "            if (not emo) and scores:\n",
        "                try:\n",
        "                    emo = max(scores, key=scores.get)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Falha ao calcular emoção dominante a partir dos scores: {e}\")\n",
        "                    emo = None\n",
        "\n",
        "            emo_score = scores.get(emo, None) if emo and scores else None\n",
        "            emo_pt = translate_emotion(emo)\n",
        "\n",
        "            # Rótulo no vídeo: pessoaX - Emoção\n",
        "            label_text = f\"{person_label} - {emo_pt or 'N/A'}\"\n",
        "            draw_label(frame, f[\"bbox\"], label_text, emo_score)\n",
        "\n",
        "            csv_data.append({\n",
        "                \"frame\": frame_idx,\n",
        "                \"face_id\": pid,\n",
        "                \"person_label\": person_label,\n",
        "                \"emotion\": emo_pt,\n",
        "                \"score\": emo_score,\n",
        "                \"caption\": caption\n",
        "            })\n",
        "\n",
        "        # Desenha a legenda (ação) em PT-BR no rodapé do frame, se existir\n",
        "        if caption:\n",
        "            try:\n",
        "                # Faixa preta semi-cheia para melhorar legibilidade\n",
        "                cv2.rectangle(frame, (10, h-40), (w-10, h-10), (0, 0, 0), -1)\n",
        "                cv2.putText(\n",
        "                    frame,\n",
        "                    str(sem_acentos(caption))[:120],\n",
        "                    (20, h-18),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.6,\n",
        "                    (255, 255, 255),\n",
        "                    2\n",
        "                )\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Falha ao desenhar legenda no frame: {e}\")\n",
        "\n",
        "        out_writer.write(frame)\n",
        "\n",
        "        frame_idx += 1\n",
        "        processed += 1\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "    cap.release()\n",
        "    out_writer.release()\n",
        "\n",
        "    end = time.time()\n",
        "    eff_fps = processed / (end - start)\n",
        "    logger.info(f\"Finalizado. {processed} frames processados. FPS efetivo: {eff_fps:.2f}\")\n",
        "\n",
        "    # Salvar CSV detalhado (emoções e ações já em PT-BR)\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(csv_data)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    logger.info(\"CSV salvo em: \" + csv_path)\n",
        "\n",
        "    # Arquivo de resumo: totalizador de emoções e ações (somente PT-BR)\n",
        "    try:\n",
        "        resumo_path = csv_path.replace(\".csv\", \"_resumo.csv\")\n",
        "\n",
        "        # Considera apenas linhas com pessoa identificada\n",
        "        df_pessoas = df[df[\"person_label\"].notna() & (df[\"person_label\"] != \"\")].copy()\n",
        "\n",
        "        # Para cada pessoa, escolhe UMA emoção representativa (mais frequente)\n",
        "        df_emoc_valid = df_pessoas[df_pessoas[\"emotion\"].notna() & (df_pessoas[\"emotion\"] != \"\")]\n",
        "        if not df_emoc_valid.empty:\n",
        "            emoc_por_pessoa = (\n",
        "                df_emoc_valid\n",
        "                .groupby(\"person_label\")[\"emotion\"]\n",
        "                .agg(lambda s: s.value_counts().idxmax())\n",
        "            )\n",
        "            emoc_counts = emoc_por_pessoa.value_counts().reset_index()\n",
        "            emoc_counts.columns = [\"item_pt_br\", \"total\"]\n",
        "            emoc_counts[\"tipo\"] = \"EMOCAO\"\n",
        "        else:\n",
        "            emoc_counts = pd.DataFrame(columns=[\"item_pt_br\", \"total\", \"tipo\"])\n",
        "\n",
        "        # Para cada pessoa, escolhe UMA ação representativa (legenda mais frequente)\n",
        "        df_actions_valid = df_pessoas[df_pessoas[\"caption\"].notna() & (df_pessoas[\"caption\"] != \"\")]\n",
        "        if not df_actions_valid.empty:\n",
        "            acao_por_pessoa = (\n",
        "                df_actions_valid\n",
        "                .groupby(\"person_label\")[\"caption\"]\n",
        "                .agg(lambda s: s.value_counts().idxmax())\n",
        "            )\n",
        "            action_counts = acao_por_pessoa.value_counts().reset_index()\n",
        "            action_counts.columns = [\"item_pt_br\", \"total\"]\n",
        "            action_counts[\"tipo\"] = \"ACAO\"\n",
        "        else:\n",
        "            action_counts = pd.DataFrame(columns=[\"item_pt_br\", \"total\", \"tipo\"])\n",
        "\n",
        "        resumo_df = pd.concat([emoc_counts, action_counts], ignore_index=True)\n",
        "        # Reorganiza colunas\n",
        "        if not resumo_df.empty:\n",
        "            resumo_df = resumo_df[[\"tipo\", \"item_pt_br\", \"total\"]]\n",
        "\n",
        "        resumo_df.to_csv(resumo_path, index=False)\n",
        "        logger.info(\"Resumo salvo em: \" + resumo_path)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Falha ao gerar arquivo de resumo: {e}\")\n",
        "\n",
        "    logger.info(\"Vídeo anotado salvo em: \" + output_path)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Ponto de entrada (linha de comando)\n",
        "# -------------------------------------------------------------------"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26-01-08 18:35:51 - Directory /root/.deepface has been created\n",
            "26-01-08 18:35:51 - Directory /root/.deepface/weights has been created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "execution_count": 3,
      "id": "m_DDWFv4CvNR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iibcGpDRCvNT"
      },
      "source": [
        "# Ajuste os caminhos abaixo conforme seu ambiente (pasta no Drive, etc.)\n",
        "folder_path = '/content/drive/MyDrive/Tech Challenge 4/'\n",
        "input_path = os.path.join(folder_path, '', 'Unlocking Facial Recognition_ Diverse Activities Analysis.mp4')\n",
        "output_video = os.path.join(folder_path, 'output', 'output_video.mp4')\n",
        "csv_output = os.path.join(folder_path, 'output', 'resultado.csv')\n",
        "\n",
        "process_video_colab(\n",
        "    input_path=input_path,\n",
        "    output_path=output_video,\n",
        "    csv_path=csv_output,\n",
        "    skip_frames=0,  # coloque 2 ou 3 se quiser acelerar\n",
        "    conf=0.5,\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "iibcGpDRCvNT"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}