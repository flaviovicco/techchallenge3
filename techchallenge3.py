# -*- coding: utf-8 -*-
"""Tech Challenge 3.ipynb

Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1-3O9hhNxlZ295mzdEI_kkMwRQdA-m0Eq
"""

from google.colab import drive
drive.mount('/content/drive')

# ------------------------------------------------------------
# Etapa 1️⃣: Preparação dos dados (carregamento e limpeza)
# ------------------------------------------------------------
# Monta o Google Drive para acesso aos arquivos.
# Define o caminho do arquivo JSON de entrada com dados brutos.
arquivo = "/content/drive/MyDrive/Tech Challenge 3/trn.json"

import pandas as pd
import json

# Lê o arquivo JSON linha a linha, limitando a 100.000 registros para reduzir uso de memória.
registros = []
max_registros = 100000

with open(arquivo, "r", encoding="utf-8") as f:
  for i, linha in enumerate(f):
    if i >= max_registros:
      break
    try:
      dado = json.loads(linha)
      registros.append(dado)
    except json.JSONDecodeError:
      continue  # ignora linhas com erro de formatação

# Converte a lista de dicionários em um DataFrame Pandas.
df = pd.DataFrame(registros)

# Mantém apenas as colunas relevantes (title e content).
df = df[["title", "content"]]

# Remove linhas com valores nulos ou vazios.
df = df.dropna(subset=["title", "content"])
df = df[(df["title"].str.strip() != "") & (df["content"].str.strip() != "")]
# Remove duplicatas baseadas no título do produto.
df = df.drop_duplicates(subset="title", keep="first")

print("Registros válidos:", len(df))
print(df.head())

# ------------------------------------------------------------
# Etapa 2️⃣: Exportar dataset limpo em formato JSONL
# ------------------------------------------------------------
arquivo_jsonl = "/content/drive/MyDrive/Tech Challenge 3/tst2.json"

# Cada linha contém um registro JSON (título e conteúdo).
with open(arquivo_jsonl, "w", encoding="utf-8") as f:
    for _, row in df.iterrows():
        json_obj = {
            "title": row["title"],
            "content": row["content"]
        }
        f.write(json.dumps(json_obj, ensure_ascii=False) + "\n")

print("Dataset exportado para JSONL com sucesso!")
print("Caminho:", arquivo_jsonl)

# ------------------------------------------------------------
# Etapa 3️⃣: Criação de resumos com modelo pré-treinado (BART)
# ------------------------------------------------------------
# Esta etapa gera resumos automáticos a partir do dataset original.
# Posteriormente, os pares (descrição → resumo) serão usados no fine-tuning.

from transformers import pipeline
import json
from tqdm import tqdm

# Carrega modelo de sumarização BART da Hugging Face.
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_amazon(jsonl_file, output_file="amazon_summaries_stream.json", flush_every=10):
    """
    Função que gera resumos automáticos a partir de descrições.
    Grava os resultados em disco de forma incremental (streaming) para economizar memória.
    """
    count = 0

    # Conta número total de linhas para exibir progresso.
    with open(jsonl_file, 'r') as f:
        total_lines = sum(1 for _ in f)

    # Leitura linha a linha e geração de resumo.
    with open(jsonl_file, 'r') as f_in, open(output_file, 'w') as f_out:
        f_out.write('{"amazon_summaries": [\n')
        first_item = True

        # tqdm fornece uma barra de progresso.
        for line in tqdm(f_in, total=total_lines, desc="Gerando resumos"):
            try:
                item = json.loads(line)
                title = item.get("title", "")
                content = item.get("content", "")
                text = f"Title: {title}\nDescription: {content}"

                # Gera o resumo com BART.
                summary = summarizer(
                    text,
                    max_length=20,
                    min_length=15,
                    do_sample=False
                )[0]["summary_text"]

            except Exception as e:
                summary = f"[ERROR: {e}]"

            # Monta o dicionário com o resumo gerado.
            result = {"title": title, "content": content, "summary": summary}

            # Grava no arquivo incrementalmente.
            if not first_item:
                f_out.write(",\n")
            json.dump(result, f_out, ensure_ascii=False)
            first_item = False

            count += 1
            if count % flush_every == 0:
                f_out.flush()  # grava no disco periodicamente

        f_out.write("\n]}\n")

    print(f"\n✅ Concluído! {count} produtos processados e salvos em {output_file}")

# Executa a geração de resumos
summarize_amazon("/content/drive/MyDrive/Tech Challenge 3/tst2.json")

# ------------------------------------------------------------
# Etapa 4️⃣: Preparar o dataset para o fine-tuning
# ------------------------------------------------------------
# Aqui o código carrega os resumos gerados e formata os dados para o treinamento
# de um modelo de linguagem (T5) via Hugging Face Transformers.

from datasets import Dataset

def load_amazon_summaries(json_file):
    with open(json_file, "r") as f:
        data = json.load(f)

    # Extrai lista de itens (title, content, summary)
    items = data["/content/drive/MyDrive/Tech Challenge 3/amazon_summaries"]

    inputs, outputs = [], []
    for item in items:
        title = item["title"]
        content = item["content"]
        summary = item["summary"]

        # Cria entradas no formato aceito pelo modelo T5:
        # o prefixo "summarize:" instrui o modelo sobre a tarefa.
        text = f"summarize: Title: {title}. Description: {content}"
        inputs.append(text)
        outputs.append(summary)

    # Cria um objeto Dataset da Hugging Face.
    return Dataset.from_dict({"input": inputs, "output": outputs})

# Carrega e divide o dataset em treino e teste.
dataset = load_amazon_summaries("/content/drive/MyDrive/Tech Challenge 3/amazon_summaries_stream.json")
dataset = dataset.train_test_split(test_size=0.2)

# ------------------------------------------------------------
# Etapa 5️⃣: Tokenização (pré-processamento textual)
# ------------------------------------------------------------
# O texto é convertido em IDs de tokens para ser interpretado pelo modelo.

from transformers import AutoTokenizer
model_name = "t5-base"  # modelo base escolhido para fine-tuning
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess(examples):
    # Tokeniza entrada (input) e saída (output/resumo)
    model_inputs = tokenizer(examples["input"], max_length=256, truncation=True, padding="max_length")
    labels = tokenizer(examples["output"], max_length=64, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Aplica a tokenização em todo o dataset
tokenized_dataset = dataset.map(preprocess, batched=True)

# ------------------------------------------------------------
# Etapa 6️⃣: Configuração e treinamento (fine-tuning)
# ------------------------------------------------------------
# Aqui ocorre o fine-tuning de fato:
# - O modelo T5-base é ajustado usando o dataset tokenizado
# - São definidas métricas, parâmetros de treino e callbacks.

from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer, GenerationConfig, DataCollatorForSeq2Seq
import evaluate

rouge = evaluate.load("rouge")  # Métrica padrão para sumarização

def compute_metrics(eval_pred):
    # Calcula as métricas ROUGE entre previsões e respostas reais.
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    return {k: v.mid.fmeasure for k, v in result.items()}

# Carrega modelo base T5
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Configura geração de texto (limite de tokens)
generation_config = GenerationConfig(max_new_tokens=64)
model.generation_config = generation_config

# Define parâmetros de treino (número de épocas, batch size, etc.)
training_args = TrainingArguments(
    output_dir="./t5-finetuned-amazon",
    do_eval=True,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=20,
)

# Responsável por unir dados e máscaras durante o treino
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)

# Cria o objeto Trainer (classe que gerencia todo o processo de fine-tuning)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Executa o treinamento (ajuste fino do modelo)
trainer.train()

# ------------------------------------------------------------
# Etapa 7️⃣: Teste do modelo fine-tunado
# ------------------------------------------------------------
# Testa o modelo com exemplos conhecidos e novos.

text = "summarize: Title: Girls Ballet Tutu Neon Pink. Description: High quality 3 layer ballet tutu. 12 inches in length"
inputs = tokenizer(text, return_tensors="pt")

outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# ------------------------------------------------------------
# Etapa 8️⃣: Salvamento do modelo ajustado
# ------------------------------------------------------------
model.save_pretrained("/content/my-t5-amazon")
tokenizer.save_pretrained("/content/my-t5-amazon")

# ------------------------------------------------------------
# Etapa 9️⃣: Validação manual e automática
# ------------------------------------------------------------
# Nesta etapa o modelo treinado é carregado e testado com
# exemplos manuais e métricas automáticas (ROUGE).

from transformers import pipeline
summarizer = pipeline("summarization", model="/content/my-t5-amazon", tokenizer="/content/my-t5-amazon")

# Validação manual — compara resumo gerado com esperado
def validate_summary(title, description, expected_summary=None):
    text = f"summarize: Title: {title}. Description: {description}"
    result = summarizer(text, max_length=50, min_length=10, do_sample=False)[0]["summary_text"]

    print("🔹 Produto:", title)
    print("📝 Resumo gerado:", result)
    if expected_summary:
        print("✅ Resumo esperado:", expected_summary)
    print("-" * 80)

# Testes com exemplos conhecidos e novos
validate_summary(
    "Girls Ballet Tutu Neon Pink",
    "High quality 3 layer ballet tutu. 12 inches in length",
    "Girls Ballet Tutu Neon Pink Description: High quality 3 layer ballet tutu."
)

validate_summary(
    "Mog's Kittens",
    "Judith Kerr’s best–selling adventures of that cat Mog have entertained children for more than 30 years...",
    "Judith Kerr's best-selling books have entertained children for more than 30 years . these sturdy little board books are just the thing to delight the very young ."
)

validate_summary(
    "Kids Soccer Ball",
    "Durable and lightweight soccer ball designed for beginners and young players.",
    None
)

# Validação automática com métrica ROUGE (avalia a qualidade dos resumos)
rouge = evaluate.load("rouge")
predictions, references = [], []

for item in dataset["test"]:
    text = f"summarize: {item['input']}"
    result = summarizer(text, max_length=50, min_length=10, do_sample=False)[0]["summary_text"]
    predictions.append(result)
    references.append(item["output"])

results = rouge.compute(predictions=predictions, references=references)

print("\n📊 Resultados ROUGE:")
for k, v in results.items():
    print(f"{k}: {v:.4f}")
