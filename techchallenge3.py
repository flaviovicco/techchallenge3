# -*- coding: utf-8 -*-
"""Tech Challenge 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-3O9hhNxlZ295mzdEI_kkMwRQdA-m0Eq
"""

from google.colab import drive
drive.mount('/content/drive')

# Caminho do arquivo no Google Drive (ap√≥s montar o drive no Colab)
arquivo = "/content/drive/MyDrive/Tech Challenge 3/trn.json"

import pandas as pd
import json

# Carrega o JSON linha a linha em uma lista
registros = []
max_registros = 100000

with open(arquivo, "r", encoding="utf-8") as f:
  for i, linha in enumerate(f):
    if i >= max_registros:
      break
    try:
      dado = json.loads(linha)
      registros.append(dado)
    except json.JSONDecodeError:
      continue

# Cria DataFrame
df = pd.DataFrame(registros)

# Mant√©m apenas colunas relevantes
df = df[["title", "content"]]

# Remove linhas com title ou content vazios/nulos
df = df.dropna(subset=["title", "content"])
df = df[(df["title"].str.strip() != "") & (df["content"].str.strip() != "")]
# Remove linhas com title duplicado
df = df.drop_duplicates(subset="title", keep="first")

print("Registros v√°lidos:", len(df))
print(df.head())

# Caminho do arquivo JSONL de sa√≠da
arquivo_jsonl = "/content/drive/MyDrive/Tech Challenge 3/tst2.json"

# Salva em JSONL (um registro por linha)
with open(arquivo_jsonl, "w", encoding="utf-8") as f:
    for _, row in df.iterrows():
        json_obj = {
            "title": row["title"],
            "content": row["content"]
        }
        f.write(json.dumps(json_obj, ensure_ascii=False) + "\n")

print("Dataset exportado para JSONL com sucesso!")
print("Caminho:", arquivo_jsonl)

!pip install transformers accelerate sentencepiece -q

from transformers import pipeline
import json

"""**Carregar modelo de sumariza√ß√£o (BART)**"""

# Carregar modelo de sumariza√ß√£o (BART)
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_amazon(jsonl_file, output_file="amazon_summaries.json"):
    summaries = []

    with open(jsonl_file, 'r') as f:
        for line in f:
            item = json.loads(line)
            title = item["title"]
            content = item["content"]

            text = f"Title: {title}\nDescription: {content}"
            summary = summarizer(
                text,
                max_length=50,   # limite m√°ximo de tokens no resumo
                min_length=15,   # m√≠nimo de tokens no resumo
                do_sample=False
            )[0]["summary_text"]

            summaries.append({
                "title": title,
                "content": content,
                "summary": summary
            })

    with open(output_file, 'w') as out:
        json.dump({"amazon_summaries": summaries}, out, indent=2)

# exemplo de chamada
summarize_amazon("/content/drive/MyDrive/Tech Challenge 3/tst2.json")

"""NOVO TESTE"""

!pip install transformers accelerate sentencepiece tqdm -q
import json
from tqdm import tqdm
from transformers import pipeline

# Inicializa o modelo (voc√™ pode trocar por "t5-base" se quiser)
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_amazon(jsonl_file, output_file="amazon_summaries_stream.json", flush_every=10):
    """
    Gera resumos a partir de um arquivo JSONL e grava em disco incrementalmente.

    Par√¢metros:
      - jsonl_file: caminho do arquivo de entrada (formato JSONL)
      - output_file: caminho do arquivo de sa√≠da (JSON)
      - flush_every: a cada quantos itens deve gravar no disco
    """
    count = 0

    # Conta quantas linhas existem para exibir progresso
    with open(jsonl_file, 'r') as f:
        total_lines = sum(1 for _ in f)

    with open(jsonl_file, 'r') as f_in, open(output_file, 'w') as f_out:
        f_out.write('{"amazon_summaries": [\n')
        first_item = True

        # tqdm mostra progresso sem carregar tudo na RAM
        for line in tqdm(f_in, total=total_lines, desc="Gerando resumos"):
            try:
                item = json.loads(line)
                title = item.get("title", "")
                content = item.get("content", "")

                text = f"Title: {title}\nDescription: {content}"

                # Gera√ß√£o do resumo
                summary = summarizer(
                    text,
                    max_length=20,
                    min_length=15,
                    do_sample=False
                )[0]["summary_text"]

            except Exception as e:
                summary = f"[ERROR: {e}]"

            # Formata o item a ser gravado
            result = {
                "title": title,
                "content": content,
                "summary": summary
            }

            if not first_item:
                f_out.write(",\n")
            json.dump(result, f_out, ensure_ascii=False)
            first_item = False

            count += 1
            if count % flush_every == 0:
                f_out.flush()  # grava no disco

        f_out.write("\n]}\n")

    print(f"\n‚úÖ Conclu√≠do! {count} produtos processados e salvos em {output_file}")

summarize_amazon("/content/drive/MyDrive/Tech Challenge 3/tst2.json")


!pip install transformers datasets evaluate rouge_score sentencepiece -q

"""**Carregar o Dataset**"""

import json
from datasets import Dataset

def load_amazon_summaries(json_file):
    with open(json_file, "r") as f:
        data = json.load(f)

    items = data["/content/drive/MyDrive/Tech Challenge 3/amazon_summaries"]

    inputs, outputs = [], []
    for item in items:
        title = item["title"]
        content = item["content"]
        summary = item["summary"]

        # Cria input estilo T5
        text = f"summarize: Title: {title}. Description: {content}"
        inputs.append(text)
        outputs.append(summary)

    return Dataset.from_dict({"input": inputs, "output": outputs})

dataset = load_amazon_summaries("/content/drive/MyDrive/Tech Challenge 3/amazon_summaries_stream.json")
dataset = dataset.train_test_split(test_size=0.2)

dataset

"""**Tokenizar**"""

from transformers import AutoTokenizer

model_name = "t5-base"  # ou "facebook/bart-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess(examples):
    model_inputs = tokenizer(examples["input"], max_length=256, truncation=True, padding="max_length")
    labels = tokenizer(examples["output"], max_length=64, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(preprocess, batched=True)

"""**Configurar modelo + treino**"""

from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer, GenerationConfig, DataCollatorForSeq2Seq
import evaluate

rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    return {k: v.mid.fmeasure for k, v in result.items()}

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Define generation configuration and pass it to the model
generation_config = GenerationConfig(max_new_tokens=64)
model.generation_config = generation_config

training_args = TrainingArguments(
    output_dir="./t5-finetuned-amazon",
    do_eval=True,                  # habilita avalia√ß√£o
    # evaluate_during_training=True, # legado das vers√µes antigas
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=20,
)

# Initialize DataCollatorForSeq2Seq with the tokenizer and model, and set label_pad_token_id
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

"""**Treinar**"""

trainer.train()

"""**Testar modelo fine-tunado**"""

text = "summarize: Title: Girls Ballet Tutu Neon Pink. Description: High quality 3 layer ballet tutu. 12 inches in length"
inputs = tokenizer(text, return_tensors="pt")

outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

"""**Salvar modelo**"""

model.save_pretrained("/content/my-t5-amazon")
tokenizer.save_pretrained("/content/my-t5-amazon")

"""**Valida√ß√£o**"""

from transformers import pipeline

# carregar modelo fine-tunado
summarizer = pipeline("summarization", model="/content/my-t5-amazon", tokenizer="/content/my-t5-amazon")

# Fun√ß√£o para validar um item
def validate_summary(title, description, expected_summary=None):
    text = f"summarize: Title: {title}. Description: {description}"
    result = summarizer(text, max_length=50, min_length=10, do_sample=False)[0]["summary_text"]

    print("üîπ Produto:", title)
    print("üìù Resumo gerado:", result)
    if expected_summary:
        print("‚úÖ Resumo esperado:", expected_summary)
    print("-" * 80)

# Teste com itens do dataset
validate_summary(
    "Girls Ballet Tutu Neon Pink",
    "High quality 3 layer ballet tutu. 12 inches in length",
    "Girls Ballet Tutu Neon Pink Description: High quality 3 layer ballet tutu."
)

validate_summary(
    "Mog's Kittens",
    "Judith Kerr‚Äôs best‚Äìselling adventures of that cat Mog have entertained children for more than 30 years...",
    "Judith Kerr's best-selling books have entertained children for more than 30 years . these sturdy little board books are just the thing to delight the very young ."
)

# Teste com produto novo (fora do dataset)
validate_summary(
    "Kids Soccer Ball",
    "Durable and lightweight soccer ball designed for beginners and young players.",
    None
)

import evaluate
from transformers import pipeline

# Carregar modelo fine-tunado
summarizer = pipeline("summarization", model="/content/my-t5-amazon", tokenizer="/content/my-t5-amazon")

# --------------------------
#  1. Valida√ß√£o manual
# --------------------------
def validate_summary(title, description, expected_summary=None):
    text = f"summarize: Title: {title}. Description: {description}"
    result = summarizer(text, max_length=50, min_length=10, do_sample=False)[0]["summary_text"]

    print("üîπ Produto:", title)
    print("üìù Resumo gerado:", result)
    if expected_summary:
        print("‚úÖ Resumo esperado:", expected_summary)
    print("-" * 80)

# Exemplos
validate_summary(
    "Girls Ballet Tutu Neon Pink",
    "High quality 3 layer ballet tutu. 12 inches in length",
    "Girls Ballet Tutu Neon Pink Description: High quality 3 layer ballet tutu."
)

validate_summary(
    "Mog's Kittens",
    "Judith Kerr‚Äôs best‚Äìselling adventures of that cat Mog have entertained children for more than 30 years...",
    "Judith Kerr's best-selling books have entertained children for more than 30 years . these sturdy little board books are just the thing to delight the very young ."
)

# Produto novo (n√£o visto no treino)
validate_summary(
    "Kids Soccer Ball",
    "Durable and lightweight soccer ball designed for beginners and young players.",
    None
)

# --------------------------
#  2. Valida√ß√£o autom√°tica com ROUGE
# --------------------------
rouge = evaluate.load("rouge")

predictions = []
references = []

for item in dataset["test"]:  # dataset carregado antes do treino
    text = f"summarize: {item['input']}"
    result = summarizer(text, max_length=50, min_length=10, do_sample=False)[0]["summary_text"]

    predictions.append(result)
    references.append(item["output"])

# Calcular m√©tricas ROUGE
results = rouge.compute(predictions=predictions, references=references)

print("\nüìä Resultados ROUGE:")
for k, v in results.items():
    print(f"{k}: {v:.4f}")
