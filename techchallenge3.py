# -*- coding: utf-8 -*-
"""Tech Challenge 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-3O9hhNxlZ295mzdEI_kkMwRQdA-m0Eq
"""

from google.colab import drive
drive.mount('/content/drive')

# Caminho do arquivo no Google Drive (após montar o drive no Colab)
arquivo = "/content/drive/MyDrive/Tech Challenge 3/trn.json"

import pandas as pd
import json

# Carrega o JSON linha a linha em uma lista
registros = []
max_registros = 100000

with open(arquivo, "r", encoding="utf-8") as f:
  for i, linha in enumerate(f):
    if i >= max_registros:
      break
    try:
      dado = json.loads(linha)
      registros.append(dado)
    except json.JSONDecodeError:
      continue

# Cria DataFrame
df = pd.DataFrame(registros)

# Mantém apenas colunas relevantes
df = df[["title", "content"]]

# Remove linhas com title ou content vazios/nulos
df = df.dropna(subset=["title", "content"])
df = df[(df["title"].str.strip() != "") & (df["content"].str.strip() != "")]
# Remove linhas com title duplicado
df = df.drop_duplicates(subset="title", keep="first")

print("Registros válidos:", len(df))
print(df.head())

# Caminho do arquivo JSONL de saída
arquivo_jsonl = "/content/drive/MyDrive/Tech Challenge 3/tst2.json"

# Salva em JSONL (um registro por linha)
with open(arquivo_jsonl, "w", encoding="utf-8") as f:
    for _, row in df.iterrows():
        json_obj = {
            "title": row["title"],
            "content": row["content"]
        }
        f.write(json.dumps(json_obj, ensure_ascii=False) + "\n")

print("Dataset exportado para JSONL com sucesso!")
print("Caminho:", arquivo_jsonl)

# !pip install ollama
# !pip install -q transformers torch

# ollama run llama2
# !ollama serve &

# !ollama serve
# !ollama start
# !ollama --version

# !curl -fsSL https://ollama.com/install.sh | sh
# !ollama serve &
# !ollama pull llama3

# !wich ollama
# !ollama --help

# import json
# import ollama

# def summarize_amazon(jsonl_file, output_file="amazon_summaries.json"):
#     summaries = []

#     with open(jsonl_file, 'r') as f:
#         for line in f:
#             item = json.loads(line)
#             title = item["title"]
#             content = item["content"]

#             prompt = f"Summarize the following Amazon product in one or two sentences:\n\nTitle: {title}\nDescription: {content}"

#             response = ollama.chat(
#                 model="llama2",  # ou 'llama3', 'mistral', dependendo do que você tiver baixado
#                 messages=[
#                     {"role": "system", "content": "You are a helpful assistant that summarizes products."},
#                     {"role": "user", "content": prompt}
#                 ]
#             )

#             summary_text = response["message"]["content"].strip()

#             summaries.append({
#                 "title": title,
#                 "content": content,
#                 "summary": summary_text
#             })

#     with open(output_file, 'w') as out:
#         json.dump({"amazon_summaries": summaries}, out, indent=2)

# # exemplo de chamada
# summarize_amazon("/content/drive/MyDrive/Tech Challenge 3/trn2.json")

# !pip install huggingface_hub -q

#!pip install transformers accelerate sentencepiece -q

# from transformers import pipeline
# import json

# # Carrega um modelo pequeno estilo LLaMA (pode trocar por outro mais forte, ex: meta-llama/Llama-2-7b-hf, mas vai pesar no Colab!)
# summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# def summarize_amazon(jsonl_file, output_file="amazon_summaries.json"):
#     summaries = []

#     with open(jsonl_file, 'r') as f:
#         for line in f:
#             item = json.loads(line)
#             title = item["title"]
#             content = item["content"]

#             text = f"Title: {title}\nDescription: {content}"
#             summary = summarizer(text, max_length=50, min_length=15, do_sample=False)[0]["summary_text"]

#             summaries.append({
#                 "title": title,
#                 "content": content,
#                 "summary": summary
#             })

#     with open(output_file, 'w') as out:
#         json.dump({"amazon_summaries": summaries}, out, indent=2)

# # exemplo de chamada
# summarize_amazon("/content/drive/MyDrive/Tech Challenge 3/trn2.json")

!pip install transformers accelerate sentencepiece -q

from transformers import pipeline
import json

"""**Carregar modelo de sumarização (BART)**"""

# Carregar modelo de sumarização (BART)
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_amazon(jsonl_file, output_file="amazon_summaries.json"):
    summaries = []

    with open(jsonl_file, 'r') as f:
        for line in f:
            item = json.loads(line)
            title = item["title"]
            content = item["content"]

            text = f"Title: {title}\nDescription: {content}"
            summary = summarizer(
                text,
                max_length=50,   # limite máximo de tokens no resumo
                min_length=15,   # mínimo de tokens no resumo
                do_sample=False
            )[0]["summary_text"]

            summaries.append({
                "title": title,
                "content": content,
                "summary": summary
            })

    with open(output_file, 'w') as out:
        json.dump({"amazon_summaries": summaries}, out, indent=2)

# exemplo de chamada
summarize_amazon("/content/drive/MyDrive/Tech Challenge 3/tst2.json")

"""NOVO TESTE"""

!pip install transformers accelerate sentencepiece tqdm -q
import json
from tqdm import tqdm
from transformers import pipeline

# Inicializa o modelo (você pode trocar por "t5-base" se quiser)
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_amazon(jsonl_file, output_file="amazon_summaries_stream.json", flush_every=10):
    """
    Gera resumos a partir de um arquivo JSONL e grava em disco incrementalmente.

    Parâmetros:
      - jsonl_file: caminho do arquivo de entrada (formato JSONL)
      - output_file: caminho do arquivo de saída (JSON)
      - flush_every: a cada quantos itens deve gravar no disco
    """
    count = 0

    # Conta quantas linhas existem para exibir progresso
    with open(jsonl_file, 'r') as f:
        total_lines = sum(1 for _ in f)

    with open(jsonl_file, 'r') as f_in, open(output_file, 'w') as f_out:
        f_out.write('{"amazon_summaries": [\n')
        first_item = True

        # tqdm mostra progresso sem carregar tudo na RAM
        for line in tqdm(f_in, total=total_lines, desc="Gerando resumos"):
            try:
                item = json.loads(line)
                title = item.get("title", "")
                content = item.get("content", "")

                text = f"Title: {title}\nDescription: {content}"

                # Geração do resumo
                summary = summarizer(
                    text,
                    max_length=20,
                    min_length=15,
                    do_sample=False
                )[0]["summary_text"]

            except Exception as e:
                summary = f"[ERROR: {e}]"

            # Formata o item a ser gravado
            result = {
                "title": title,
                "content": content,
                "summary": summary
            }

            if not first_item:
                f_out.write(",\n")
            json.dump(result, f_out, ensure_ascii=False)
            first_item = False

            count += 1
            if count % flush_every == 0:
                f_out.flush()  # grava no disco

        f_out.write("\n]}\n")

    print(f"\n✅ Concluído! {count} produtos processados e salvos em {output_file}")

summarize_amazon("/content/drive/MyDrive/Tech Challenge 3/tst2.json")

# #!pip install transformers accelerate sentencepiece -q

# from transformers import pipeline
# import json

# # Carregar modelo de sumarização (T5-base)
# summarizer = pipeline("summarization", model="t5-base")

# def summarize_amazon(jsonl_file, output_file="/content/drive/MyDrive/Tech Challenge 3/amazon_summaries_test.json", limit=5):
#     summaries = []

#     with open(jsonl_file, 'r') as f:
#         for i, line in enumerate(f):
#             if i >= limit:  # só processa os primeiros N itens
#                 break
#             item = json.loads(line)
#             title = item["title"]
#             content = item["content"]

#             text = f"summarize: Title: {title}\nDescription: {content}"
#             summary = summarizer(
#                 text,
#                 max_length=20,   # limite máximo de tokens no resumo
#                 min_length=10,   # mínimo de tokens no resumo
#                 do_sample=False
#             )[0]["summary_text"]

#             summaries.append({
#                 "title": title,
#                 "content": content,
#                 "summary": summary
#             })

#     with open(output_file, 'w') as out:
#         json.dump({"/content/drive/MyDrive/Tech Challenge 3/amazon_summaries": summaries}, out, indent=2)

#     print(f"Arquivo salvo em {output_file}")

# # exemplo de chamada (teste com 5 produtos)
# summarize_amazon("/content/drive/MyDrive/Tech Challenge 3/tst2.json", limit=5)

"""***Executando Este***"""

# #!pip install transformers accelerate sentencepiece -q

# from transformers import pipeline
# import json

# # Carregar modelo de sumarização (T5-base)
# summarizer = pipeline("summarization", model="t5-base")

# def summarize_amazon(jsonl_file, output_file="/content/drive/MyDrive/Tech Challenge 3/amazon_summaries.json"):
#     summaries = []

#     with open(jsonl_file, 'r') as f:
#         for line in f:
#             item = json.loads(line)
#             title = item["title"]
#             content = item["content"]

#             text = f"summarize: Title: {title}\nDescription: {content}"
#             summary = summarizer(
#                 text,
#                 max_length=20,   # limite máximo de tokens no resumo
#                 min_length=10,   # mínimo de tokens no resumo
#                 do_sample=False
#             )[0]["summary_text"]

#             summaries.append({
#                 "title": title,
#                 "content": content,
#                 "summary": summary
#             })

#     with open(output_file, 'w') as out:
#         json.dump({"/content/drive/MyDrive/Tech Challenge 3/amazon_summaries": summaries}, out, indent=2)

#     print(f"✅ Resumos gerados e salvos em {output_file}")

# # Executar para todos os itens
# summarize_amazon("/content/drive/MyDrive/Tech Challenge 3/tst2.json")

# !pip install huggingface_hub -q

# import json
# from huggingface_hub import InferenceClient
# from google.colab import userdata

# # Criar cliente da API do HuggingFace
# # Get the API key from Colab secrets
# hf_api_key = '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' #userdata.get('HF_TOKEN')

# if not hf_api_key:
#     print("Error: Hugging Face API key not found in Colab secrets. Please add it as 'HF_TOKEN'.")
# else:
#     client = InferenceClient(api_key=hf_api_key)

#     def summarize_news(news_file, output_file="news_summaries.json", model="meta-llama/Llama-2-7b-chat-hf"):
#         # Carrega o conteúdo das notícias de um arquivo JSON
#         try:
#             with open(news_file, 'r') as file:
#                 news_data = json.load(file)
#                 # Check if 'news_content' key exists and is a list
#                 if 'news_content' in news_data and isinstance(news_data['news_content'], list):
#                     news_contents = news_data['news_content']
#                 else:
#                     print(f"Error: 'news_content' key not found or is not a list in {news_file}")
#                     return
#         except FileNotFoundError:
#             print(f"Error: File not found at {news_file}")
#             return
#         except json.JSONDecodeError:
#             print(f"Error: Could not decode JSON from {news_file}")
#             return


#         summaries = []

#         for content in news_contents:
#             prompt = f"Summarize the following news article in 2-3 sentences:\n\n{content}"

#             try:
#                 # Chama a Inference API do HuggingFace
#                 response = client.text_generation(
#                     model=model,
#                     prompt=prompt,
#                     max_new_tokens=120,
#                     temperature=0.7
#                 )

#                 summary_text = response.strip()
#                 print(summary_text)

#                 summaries.append({
#                     "story": content,
#                     "summary": summary_text
#                 })
#             except Exception as e:
#                 print(f"Error summarizing content: {e}")
#                 summaries.append({
#                     "story": content,
#                     "summary": f"Error summarizing: {e}"
#                 })


#         # Salva os resultados em um arquivo JSON
#         try:
#             with open(output_file, 'w') as json_file:
#                 json.dump({"news_summaries": summaries}, json_file, indent=2)
#             print(f"✅ Resumos salvos em {output_file}")
#         except IOError as e:
#             print(f"Error saving summaries to {output_file}: {e}")


#     # Exemplo de chamada
#     summarize_news('/content/drive/MyDrive/news_contents.json')

!pip install transformers datasets evaluate rouge_score sentencepiece -q

"""**Carregar o Dataset**"""

import json
from datasets import Dataset

def load_amazon_summaries(json_file):
    with open(json_file, "r") as f:
        data = json.load(f)

    items = data["/content/drive/MyDrive/Tech Challenge 3/amazon_summaries"]

    inputs, outputs = [], []
    for item in items:
        title = item["title"]
        content = item["content"]
        summary = item["summary"]

        # Cria input estilo T5
        text = f"summarize: Title: {title}. Description: {content}"
        inputs.append(text)
        outputs.append(summary)

    return Dataset.from_dict({"input": inputs, "output": outputs})

dataset = load_amazon_summaries("/content/drive/MyDrive/Tech Challenge 3/amazon_summaries_stream.json")
dataset = dataset.train_test_split(test_size=0.2)

dataset

"""**Tokenizar**"""

from transformers import AutoTokenizer

model_name = "t5-base"  # ou "facebook/bart-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess(examples):
    model_inputs = tokenizer(examples["input"], max_length=256, truncation=True, padding="max_length")
    labels = tokenizer(examples["output"], max_length=64, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(preprocess, batched=True)

"""**Configurar modelo + treino**"""

from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer, GenerationConfig, DataCollatorForSeq2Seq
import evaluate

rouge = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    return {k: v.mid.fmeasure for k, v in result.items()}

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Define generation configuration and pass it to the model
generation_config = GenerationConfig(max_new_tokens=64)
model.generation_config = generation_config

training_args = TrainingArguments(
    output_dir="./t5-finetuned-amazon",
    do_eval=True,                  # habilita avaliação
    # evaluate_during_training=True, # legado das versões antigas
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=20,
)

# Initialize DataCollatorForSeq2Seq with the tokenizer and model, and set label_pad_token_id
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=tokenizer.pad_token_id)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

"""**Treinar**"""

trainer.train()

"""**Testar modelo fine-tunado**"""

text = "summarize: Title: Girls Ballet Tutu Neon Pink. Description: High quality 3 layer ballet tutu. 12 inches in length"
inputs = tokenizer(text, return_tensors="pt")

outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

"""**Salvar modelo**"""

model.save_pretrained("/content/my-t5-amazon")
tokenizer.save_pretrained("/content/my-t5-amazon")

"""**Validação**"""

from transformers import pipeline

# carregar modelo fine-tunado
summarizer = pipeline("summarization", model="/content/my-t5-amazon", tokenizer="/content/my-t5-amazon")

# Função para validar um item
def validate_summary(title, description, expected_summary=None):
    text = f"summarize: Title: {title}. Description: {description}"
    result = summarizer(text, max_length=50, min_length=10, do_sample=False)[0]["summary_text"]

    print("🔹 Produto:", title)
    print("📝 Resumo gerado:", result)
    if expected_summary:
        print("✅ Resumo esperado:", expected_summary)
    print("-" * 80)

# Teste com itens do dataset
validate_summary(
    "Girls Ballet Tutu Neon Pink",
    "High quality 3 layer ballet tutu. 12 inches in length",
    "Girls Ballet Tutu Neon Pink Description: High quality 3 layer ballet tutu."
)

validate_summary(
    "Mog's Kittens",
    "Judith Kerr’s best–selling adventures of that cat Mog have entertained children for more than 30 years...",
    "Judith Kerr's best-selling books have entertained children for more than 30 years . these sturdy little board books are just the thing to delight the very young ."
)

# Teste com produto novo (fora do dataset)
validate_summary(
    "Kids Soccer Ball",
    "Durable and lightweight soccer ball designed for beginners and young players.",
    None
)

import evaluate
from transformers import pipeline

# Carregar modelo fine-tunado
summarizer = pipeline("summarization", model="/content/my-t5-amazon", tokenizer="/content/my-t5-amazon")

# --------------------------
# 🔸 1. Validação manual
# --------------------------
def validate_summary(title, description, expected_summary=None):
    text = f"summarize: Title: {title}. Description: {description}"
    result = summarizer(text, max_length=50, min_length=10, do_sample=False)[0]["summary_text"]

    print("🔹 Produto:", title)
    print("📝 Resumo gerado:", result)
    if expected_summary:
        print("✅ Resumo esperado:", expected_summary)
    print("-" * 80)

# Exemplos
validate_summary(
    "Girls Ballet Tutu Neon Pink",
    "High quality 3 layer ballet tutu. 12 inches in length",
    "Girls Ballet Tutu Neon Pink Description: High quality 3 layer ballet tutu."
)

validate_summary(
    "Mog's Kittens",
    "Judith Kerr’s best–selling adventures of that cat Mog have entertained children for more than 30 years...",
    "Judith Kerr's best-selling books have entertained children for more than 30 years . these sturdy little board books are just the thing to delight the very young ."
)

# Produto novo (não visto no treino)
validate_summary(
    "Kids Soccer Ball",
    "Durable and lightweight soccer ball designed for beginners and young players.",
    None
)

# --------------------------
# 🔸 2. Validação automática com ROUGE
# --------------------------
rouge = evaluate.load("rouge")

predictions = []
references = []

for item in dataset["test"]:  # dataset carregado antes do treino
    text = f"summarize: {item['input']}"
    result = summarizer(text, max_length=50, min_length=10, do_sample=False)[0]["summary_text"]

    predictions.append(result)
    references.append(item["output"])

# Calcular métricas ROUGE
results = rouge.compute(predictions=predictions, references=references)

print("\n📊 Resultados ROUGE:")
for k, v in results.items():
    print(f"{k}: {v:.4f}")